{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0cb93f-35df-4476-9e51-43a43e16b079",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Employee ID  Age  Gender  Years at Company    Job Role  Monthly Income  \\\n",
       " 0         8410   31    Male                19   Education            5390   \n",
       " 1        64756   59  Female                 4       Media            5534   \n",
       " 2        30257   24  Female                10  Healthcare            8159   \n",
       " 3        65791   36  Female                 7   Education            3989   \n",
       " 4        65026   56    Male                41   Education            4821   \n",
       " \n",
       "   Work-Life Balance Job Satisfaction Performance Rating  Number of Promotions  \\\n",
       " 0         Excellent           Medium            Average                     2   \n",
       " 1              Poor             High                Low                     3   \n",
       " 2              Good             High                Low                     0   \n",
       " 3              Good             High               High                     1   \n",
       " 4     Below Average        Very High            Average                     0   \n",
       " \n",
       "    ... Company Size  Company Tenure Remote Work Leadership Opportunities  \\\n",
       " 0  ...       Medium              89          No                       No   \n",
       " 1  ...       Medium              21          No                       No   \n",
       " 2  ...       Medium              74          No                       No   \n",
       " 3  ...        Small              50         Yes                       No   \n",
       " 4  ...       Medium              68          No                       No   \n",
       " \n",
       "    Innovation Opportunities Company Reputation Employee Recognition  \\\n",
       " 0                        No          Excellent               Medium   \n",
       " 1                        No               Fair                  Low   \n",
       " 2                        No               Poor                  Low   \n",
       " 3                        No               Good               Medium   \n",
       " 4                        No               Fair               Medium   \n",
       " \n",
       "    Attrition Overall_Sentiment_Score Overall_Sentiment_Label  \n",
       " 0     Stayed                 0.28595                Positive  \n",
       " 1     Stayed                -0.05452                Negative  \n",
       " 2     Stayed                -0.10957                Negative  \n",
       " 3     Stayed                 0.22020                Positive  \n",
       " 4     Stayed                 0.15910                Positive  \n",
       " \n",
       " [5 rows x 26 columns],\n",
       "    Employee ID  Age Gender  Years at Company    Job Role  Monthly Income  \\\n",
       " 0        52685   36   Male                13  Healthcare            8029   \n",
       " 1        30585   35   Male                 7   Education            4563   \n",
       " 2        54656   50   Male                 7   Education            5583   \n",
       " 3        33442   58   Male                44       Media            5525   \n",
       " 4        15667   39   Male                24   Education            4604   \n",
       " \n",
       "   Work-Life Balance Job Satisfaction Performance Rating  Number of Promotions  \\\n",
       " 0         Excellent             High            Average                     1   \n",
       " 1              Good             High            Average                     1   \n",
       " 2     Below Average             High            Average                     3   \n",
       " 3     Below Average        Very High               High                     0   \n",
       " 4              Good             High            Average                     0   \n",
       " \n",
       "    ... Company Size  Company Tenure Remote Work Leadership Opportunities  \\\n",
       " 0  ...        Large              22          No                       No   \n",
       " 1  ...       Medium              27          No                       No   \n",
       " 2  ...       Medium              76          No                       No   \n",
       " 3  ...       Medium              96          No                       No   \n",
       " 4  ...        Large              45         Yes                       No   \n",
       " \n",
       "    Innovation Opportunities Company Reputation Employee Recognition  \\\n",
       " 0                        No               Poor               Medium   \n",
       " 1                        No               Good                 High   \n",
       " 2                       Yes               Good                  Low   \n",
       " 3                        No               Poor                  Low   \n",
       " 4                        No               Good                 High   \n",
       " \n",
       "    Attrition Overall_Sentiment_Score Overall_Sentiment_Label  \n",
       " 0     Stayed                -0.02863                 Neutral  \n",
       " 1       Left                 0.22020                Positive  \n",
       " 2     Stayed                 0.14112                Positive  \n",
       " 3       Left                -0.13401                Negative  \n",
       " 4     Stayed                 0.22020                Positive  \n",
       " \n",
       " [5 rows x 26 columns])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_file_path = \"new_review_train.csv\"\n",
    "test_file_path = \"new_review_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "train_df.head(), test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf7dfb6-9ddb-4b3e-89da-98fa9f31698b",
   "metadata": {},
   "source": [
    "##  Step 1: Preprocessing the Data.\n",
    "### ✅ Encoded categorical features\n",
    "### ✅ Normalized numerical features\n",
    "### ✅ Fixed encoding for \"Job Level\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4530a64-241c-4a20-b2ca-129d39d27c3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Mid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9340\\3545634753.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mlabel_encoders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Normalize numerical features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumerical_features\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumerical_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumerical_features\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumerical_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Check the processed data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             return_tuple = (\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    914\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    890\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \"\"\"\n\u001b[0;32m    892\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1385\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1386\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1387\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1388\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1389\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    926\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m         \"\"\"\n\u001b[0;32m    929\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"n_samples_seen_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         X = validate_data(\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2940\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2941\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2944\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2945\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2947\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1052\u001b[0m                         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m                 raise ValueError(\n\u001b[0;32m   1058\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m                 \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    828\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 832\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     def __array__(\n\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Mid'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = ['Gender', 'Job Role', 'Work-Life Balance', 'Job Satisfaction',\n",
    "                        'Performance Rating', 'Overtime', 'Education Level', 'Marital Status',\n",
    "                        'Company Size', 'Company Reputation', 'Employee Recognition', \n",
    "                        'Remote Work', 'Leadership Opportunities', 'Innovation Opportunities',\n",
    "                        'Overall_Sentiment_Label']\n",
    "\n",
    "numerical_features = ['Age', 'Years at Company', 'Monthly Income', 'Number of Promotions',\n",
    "                      'Distance from Home', 'Job Level', 'Company Tenure', 'Overall_Sentiment_Score']\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "train_df[numerical_features] = scaler.fit_transform(train_df[numerical_features])\n",
    "test_df[numerical_features] = scaler.transform(test_df[numerical_features])\n",
    "\n",
    "# Check the processed data\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28561bb-bdba-4040-9f52-f630383753a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd8d04-a36f-4cc2-86dc-7f12a65207f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for non-numeric values in numerical columns\n",
    "for col in numerical_features:\n",
    "    if not pd.api.types.is_numeric_dtype(train_df[col]):\n",
    "        print(f\"Non-numeric values found in column: {col}\")\n",
    "        print(train_df[col].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fddd40e-0a2b-4d0c-a5de-d32976cd9b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Years at Company</th>\n",
       "      <th>Job Role</th>\n",
       "      <th>Monthly Income</th>\n",
       "      <th>Work-Life Balance</th>\n",
       "      <th>Job Satisfaction</th>\n",
       "      <th>Performance Rating</th>\n",
       "      <th>Number of Promotions</th>\n",
       "      <th>...</th>\n",
       "      <th>Company Size</th>\n",
       "      <th>Company Tenure</th>\n",
       "      <th>Remote Work</th>\n",
       "      <th>Leadership Opportunities</th>\n",
       "      <th>Innovation Opportunities</th>\n",
       "      <th>Company Reputation</th>\n",
       "      <th>Employee Recognition</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>Overall_Sentiment_Score</th>\n",
       "      <th>Overall_Sentiment_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8410</td>\n",
       "      <td>-0.626336</td>\n",
       "      <td>1</td>\n",
       "      <td>0.288648</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.888892</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.173308</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.308164</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>1.362105</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64756</td>\n",
       "      <td>1.691627</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.045173</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.821960</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.178351</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.367856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>-1.213942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30257</td>\n",
       "      <td>-1.205827</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.511645</td>\n",
       "      <td>2</td>\n",
       "      <td>0.398153</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.836777</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.717865</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>-1.630458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65791</td>\n",
       "      <td>-0.212414</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.778409</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.540084</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.168266</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.226612</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>0.864631</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65026</td>\n",
       "      <td>1.443273</td>\n",
       "      <td>1</td>\n",
       "      <td>2.244918</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.153366</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.836777</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.481746</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>0.402339</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Employee ID       Age  Gender  Years at Company  Job Role  Monthly Income  \\\n",
       "0         8410 -0.626336       1          0.288648         0       -0.888892   \n",
       "1        64756  1.691627       0         -1.045173         3       -0.821960   \n",
       "2        30257 -1.205827       0         -0.511645         2        0.398153   \n",
       "3        65791 -0.212414       0         -0.778409         0       -1.540084   \n",
       "4        65026  1.443273       1          2.244918         0       -1.153366   \n",
       "\n",
       "   Work-Life Balance  Job Satisfaction  Performance Rating  \\\n",
       "0                  1                 2                   0   \n",
       "1                  3                 0                   3   \n",
       "2                  2                 0                   3   \n",
       "3                  2                 0                   2   \n",
       "4                  0                 3                   0   \n",
       "\n",
       "   Number of Promotions  ...  Company Size  Company Tenure  Remote Work  \\\n",
       "0              1.173308  ...             1        1.308164            0   \n",
       "1              2.178351  ...             1       -1.367856            0   \n",
       "2             -0.836777  ...             1        0.717865            0   \n",
       "3              0.168266  ...             2       -0.226612            1   \n",
       "4             -0.836777  ...             1        0.481746            0   \n",
       "\n",
       "   Leadership Opportunities  Innovation Opportunities  Company Reputation  \\\n",
       "0                         0                         0                   0   \n",
       "1                         0                         0                   1   \n",
       "2                         0                         0                   3   \n",
       "3                         0                         0                   2   \n",
       "4                         0                         0                   1   \n",
       "\n",
       "   Employee Recognition  Attrition  Overall_Sentiment_Score  \\\n",
       "0                     2     Stayed                 1.362105   \n",
       "1                     1     Stayed                -1.213942   \n",
       "2                     1     Stayed                -1.630458   \n",
       "3                     2     Stayed                 0.864631   \n",
       "4                     2     Stayed                 0.402339   \n",
       "\n",
       "   Overall_Sentiment_Label  \n",
       "0                        2  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        2  \n",
       "4                        2  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode \"Job Level\" column\n",
    "le_job_level = LabelEncoder()\n",
    "train_df[\"Job Level\"] = le_job_level.fit_transform(train_df[\"Job Level\"])\n",
    "test_df[\"Job Level\"] = le_job_level.transform(test_df[\"Job Level\"])\n",
    "\n",
    "# Reattempt normalization for numerical features\n",
    "train_df[numerical_features] = scaler.fit_transform(train_df[numerical_features])\n",
    "test_df[numerical_features] = scaler.transform(test_df[numerical_features])\n",
    "\n",
    "# Check processed data\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d1e15-d288-49f0-9d27-6028929fabf1",
   "metadata": {},
   "source": [
    "## Step 2: Apply MCDM Feature Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ede7bb6-2559-4948-bf53-20c022c04b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Years at Company</th>\n",
       "      <th>Job Role</th>\n",
       "      <th>Monthly Income</th>\n",
       "      <th>Work-Life Balance</th>\n",
       "      <th>Job Satisfaction</th>\n",
       "      <th>Performance Rating</th>\n",
       "      <th>Number of Promotions</th>\n",
       "      <th>...</th>\n",
       "      <th>Company Size</th>\n",
       "      <th>Company Tenure</th>\n",
       "      <th>Remote Work</th>\n",
       "      <th>Leadership Opportunities</th>\n",
       "      <th>Innovation Opportunities</th>\n",
       "      <th>Company Reputation</th>\n",
       "      <th>Employee Recognition</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>Overall_Sentiment_Score</th>\n",
       "      <th>Overall_Sentiment_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8410</td>\n",
       "      <td>-0.626336</td>\n",
       "      <td>1</td>\n",
       "      <td>0.063433</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.166872</td>\n",
       "      <td>0.29356</td>\n",
       "      <td>0.438396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058933</td>\n",
       "      <td>0.618416</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150518</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>0.220111</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64756</td>\n",
       "      <td>1.691627</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.229688</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.154307</td>\n",
       "      <td>0.88068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623586</td>\n",
       "      <td>0.645777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058933</td>\n",
       "      <td>-0.646635</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.255356</td>\n",
       "      <td>0.075259</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>-0.196168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30257</td>\n",
       "      <td>-1.205827</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.112440</td>\n",
       "      <td>2</td>\n",
       "      <td>0.074745</td>\n",
       "      <td>0.58712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623586</td>\n",
       "      <td>-0.248064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058933</td>\n",
       "      <td>0.339361</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766068</td>\n",
       "      <td>0.075259</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>-0.263476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65791</td>\n",
       "      <td>-0.212414</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.171064</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.289120</td>\n",
       "      <td>0.58712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.415724</td>\n",
       "      <td>0.049883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117866</td>\n",
       "      <td>-0.107128</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.510712</td>\n",
       "      <td>0.150518</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>0.139721</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65026</td>\n",
       "      <td>1.443273</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493346</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.216521</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.657594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.248064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058933</td>\n",
       "      <td>0.227739</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.255356</td>\n",
       "      <td>0.150518</td>\n",
       "      <td>Stayed</td>\n",
       "      <td>0.065016</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Employee ID       Age  Gender  Years at Company  Job Role  Monthly Income  \\\n",
       "0         8410 -0.626336       1          0.063433         0       -0.166872   \n",
       "1        64756  1.691627       0         -0.229688         3       -0.154307   \n",
       "2        30257 -1.205827       0         -0.112440         2        0.074745   \n",
       "3        65791 -0.212414       0         -0.171064         0       -0.289120   \n",
       "4        65026  1.443273       1          0.493346         0       -0.216521   \n",
       "\n",
       "   Work-Life Balance  Job Satisfaction  Performance Rating  \\\n",
       "0            0.29356          0.438396            0.000000   \n",
       "1            0.88068          0.000000            0.623586   \n",
       "2            0.58712          0.000000            0.623586   \n",
       "3            0.58712          0.000000            0.415724   \n",
       "4            0.00000          0.657594            0.000000   \n",
       "\n",
       "   Number of Promotions  ...  Company Size  Company Tenure  Remote Work  \\\n",
       "0              0.347830  ...      0.058933        0.618416            0   \n",
       "1              0.645777  ...      0.058933       -0.646635            0   \n",
       "2             -0.248064  ...      0.058933        0.339361            0   \n",
       "3              0.049883  ...      0.117866       -0.107128            1   \n",
       "4             -0.248064  ...      0.058933        0.227739            0   \n",
       "\n",
       "   Leadership Opportunities  Innovation Opportunities  Company Reputation  \\\n",
       "0                         0                         0            0.000000   \n",
       "1                         0                         0            0.255356   \n",
       "2                         0                         0            0.766068   \n",
       "3                         0                         0            0.510712   \n",
       "4                         0                         0            0.255356   \n",
       "\n",
       "   Employee Recognition  Attrition  Overall_Sentiment_Score  \\\n",
       "0              0.150518     Stayed                 0.220111   \n",
       "1              0.075259     Stayed                -0.196168   \n",
       "2              0.075259     Stayed                -0.263476   \n",
       "3              0.150518     Stayed                 0.139721   \n",
       "4              0.150518     Stayed                 0.065016   \n",
       "\n",
       "   Overall_Sentiment_Label  \n",
       "0                        2  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        2  \n",
       "4                        2  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the MCDM feature weights\n",
    "mcdm_weights = {\n",
    "    'Years at Company': 0.219761,\n",
    "    'Monthly Income': 0.18773,\n",
    "    'Work-Life Balance': 0.293560,\n",
    "    'Job Satisfaction': 0.219198,\n",
    "    'Performance Rating': 0.207862,\n",
    "    'Number of Promotions': 0.296452,\n",
    "    'Distance from Home': 0.528879,\n",
    "    'Education Level': 0.152396,\n",
    "    'Job Level': 0.135266,\n",
    "    'Company Size': 0.058933,\n",
    "    'Company Tenure': 0.472736,\n",
    "    'Company Reputation': 0.255356,\n",
    "    'Employee Recognition': 0.075259,\n",
    "    'Overall_Sentiment_Score': 0.161596\n",
    "}\n",
    "\n",
    "# Apply weights to the relevant features in train and test datasets\n",
    "for feature, weight in mcdm_weights.items():\n",
    "    train_df[feature] *= weight\n",
    "    test_df[feature] *= weight\n",
    "\n",
    "# Check the modified dataset\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc51e3f-71ff-4c0f-b6b1-3b033ddea14b",
   "metadata": {},
   "source": [
    "## Step 3: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c1de15-17a0-4710-b3ec-b8adbf75d35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59598, 24), (59598,), (14900, 24), (14900,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert target column to binary (1 = Left, 0 = Stayed)\n",
    "train_df[\"Attrition\"] = train_df[\"Attrition\"].apply(lambda x: 1 if x == \"Left\" else 0)\n",
    "test_df[\"Attrition\"] = test_df[\"Attrition\"].apply(lambda x: 1 if x == \"Left\" else 0)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X_train = train_df.drop(columns=[\"Attrition\", \"Employee ID\"])\n",
    "y_train = train_df[\"Attrition\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"Attrition\", \"Employee ID\"])\n",
    "y_test = test_df[\"Attrition\"]\n",
    "\n",
    "# Final check of shapes\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a0df8-b33e-4811-a76b-dc20f406e514",
   "metadata": {},
   "source": [
    "### Step 4:  Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c994f7c7-2c82-4da3-88dc-e758effff7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy: 0.7513\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fc249e-959c-45c1-9f63-061bf33cf273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7513\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb1525-48b8-4f91-bba4-ddcc7f90655d",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf5439a-cabc-4f06-be3d-b8326700ffed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (XGBoost): 0.7821\n",
      "Testing Accuracy (XGBoost): 0.7602\n",
      "\n",
      "Classification Report (Training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79     31260\n",
      "           1       0.77      0.77      0.77     28338\n",
      "\n",
      "    accuracy                           0.78     59598\n",
      "   macro avg       0.78      0.78      0.78     59598\n",
      "weighted avg       0.78      0.78      0.78     59598\n",
      "\n",
      "Classification Report (Testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      7868\n",
      "           1       0.74      0.75      0.75      7032\n",
      "\n",
      "    accuracy                           0.76     14900\n",
      "   macro avg       0.76      0.76      0.76     14900\n",
      "weighted avg       0.76      0.76      0.76     14900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the XGBoost model (Boosting technique)\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy_xgb = accuracy_score(y_train, y_train_pred_xgb)\n",
    "test_accuracy_xgb = accuracy_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "# Classification reports\n",
    "train_report_xgb = classification_report(y_train, y_train_pred_xgb)\n",
    "test_report_xgb = classification_report(y_test, y_test_pred_xgb)\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Accuracy (XGBoost): {train_accuracy_xgb:.4f}\")\n",
    "print(f\"Testing Accuracy (XGBoost): {test_accuracy_xgb:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report (Training):\")\n",
    "print(train_report_xgb)\n",
    "\n",
    "print(\"Classification Report (Testing):\")\n",
    "print(test_report_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cafd38c-f2d6-4205-b0d2-c952cf8f870c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7602013422818792,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.77      0.77      0.77      7868\\n           1       0.74      0.75      0.75      7032\\n\\n    accuracy                           0.76     14900\\n   macro avg       0.76      0.76      0.76     14900\\nweighted avg       0.76      0.76      0.76     14900\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost model (Boosting technique)\n",
    "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "classification_report_xgb = classification_report(y_test, y_pred_xgb)\n",
    "\n",
    "accuracy_xgb, classification_report_xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a210d7-6dca-4428-b2c8-eb425b18c92a",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71789ba9-cc75-46b5-bf78-7ca06af63163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 28338, number of negative: 31260\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 693\n",
      "[LightGBM] [Info] Number of data points in the train set: 59598, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.475486 -> initscore=-0.098136\n",
      "[LightGBM] [Info] Start training from score -0.098136\n",
      "Training Accuracy (LightGBM): 0.7728\n",
      "Testing Accuracy (LightGBM): 0.7585\n",
      "\n",
      "Classification Report (Training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     31260\n",
      "           1       0.76      0.76      0.76     28338\n",
      "\n",
      "    accuracy                           0.77     59598\n",
      "   macro avg       0.77      0.77      0.77     59598\n",
      "weighted avg       0.77      0.77      0.77     59598\n",
      "\n",
      "Classification Report (Testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      7868\n",
      "           1       0.74      0.75      0.74      7032\n",
      "\n",
      "    accuracy                           0.76     14900\n",
      "   macro avg       0.76      0.76      0.76     14900\n",
      "weighted avg       0.76      0.76      0.76     14900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the LightGBM model (Boosting technique)\n",
    "lgbm_model = LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_lgbm = lgbm_model.predict(X_train)\n",
    "y_test_pred_lgbm = lgbm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy_lgbm = accuracy_score(y_train, y_train_pred_lgbm)\n",
    "test_accuracy_lgbm = accuracy_score(y_test, y_test_pred_lgbm)\n",
    "\n",
    "# Classification reports\n",
    "train_report_lgbm = classification_report(y_train, y_train_pred_lgbm)\n",
    "test_report_lgbm = classification_report(y_test, y_test_pred_lgbm)\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Accuracy (LightGBM): {train_accuracy_lgbm:.4f}\")\n",
    "print(f\"Testing Accuracy (LightGBM): {test_accuracy_lgbm:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report (Training):\")\n",
    "print(train_report_lgbm)\n",
    "\n",
    "print(\"Classification Report (Testing):\")\n",
    "print(test_report_lgbm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc87698a-51a9-46b7-a09e-677ca5b09727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 28338, number of negative: 31260\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 693\n",
      "[LightGBM] [Info] Number of data points in the train set: 59598, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.475486 -> initscore=-0.098136\n",
      "[LightGBM] [Info] Start training from score -0.098136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7585234899328859,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.77      0.77      0.77      7868\\n           1       0.74      0.75      0.74      7032\\n\\n    accuracy                           0.76     14900\\n   macro avg       0.76      0.76      0.76     14900\\nweighted avg       0.76      0.76      0.76     14900\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Initialize the LightGBM model (Boosting technique)\n",
    "lgbm_model = LGBMClassifier(n_estimators=100, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lgbm = lgbm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_lgbm = accuracy_score(y_test, y_pred_lgbm)\n",
    "classification_report_lgbm = classification_report(y_test, y_pred_lgbm)\n",
    "\n",
    "accuracy_lgbm, classification_report_lgbm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678d57c-f77c-461f-bc51-bd0048df0c2c",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b3d00f-4c7d-427a-bc2b-77106ef2871a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (AdaBoost): 0.7102\n",
      "Testing Accuracy (AdaBoost): 0.7146\n",
      "\n",
      "Classification Report (Training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.72     31260\n",
      "           1       0.70      0.69      0.69     28338\n",
      "\n",
      "    accuracy                           0.71     59598\n",
      "   macro avg       0.71      0.71      0.71     59598\n",
      "weighted avg       0.71      0.71      0.71     59598\n",
      "\n",
      "Classification Report (Testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73      7868\n",
      "           1       0.70      0.69      0.70      7032\n",
      "\n",
      "    accuracy                           0.71     14900\n",
      "   macro avg       0.71      0.71      0.71     14900\n",
      "weighted avg       0.71      0.71      0.71     14900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_ada = adaboost_model.predict(X_train)\n",
    "y_test_pred_ada = adaboost_model.predict(X_test)\n",
    "\n",
    "# Accuracy scores\n",
    "train_accuracy_ada = accuracy_score(y_train, y_train_pred_ada)\n",
    "test_accuracy_ada = accuracy_score(y_test, y_test_pred_ada)\n",
    "\n",
    "# Classification reports\n",
    "train_report_ada = classification_report(y_train, y_train_pred_ada)\n",
    "test_report_ada = classification_report(y_test, y_test_pred_ada)\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Accuracy (AdaBoost): {train_accuracy_ada:.4f}\")\n",
    "print(f\"Testing Accuracy (AdaBoost): {test_accuracy_ada:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report (Training):\")\n",
    "print(train_report_ada)\n",
    "\n",
    "print(\"Classification Report (Testing):\")\n",
    "print(test_report_ada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e508c042-48ca-437c-92da-8d8d706a3aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7145637583892618,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.73      0.73      0.73      7868\\n           1       0.70      0.69      0.70      7032\\n\\n    accuracy                           0.71     14900\\n   macro avg       0.71      0.71      0.71     14900\\nweighted avg       0.71      0.71      0.71     14900\\n')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initialize the AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_adaboost = adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n",
    "classification_report_adaboost = classification_report(y_test, y_pred_adaboost)\n",
    "\n",
    "accuracy_adaboost, classification_report_adaboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a9a0704-0b93-4bee-824e-cef7ec5b10c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (Decision Tree): 1.0000\n",
      "Testing Accuracy (Decision Tree): 0.6657\n",
      "\n",
      "Classification Report (Training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     31260\n",
      "           1       1.00      1.00      1.00     28338\n",
      "\n",
      "    accuracy                           1.00     59598\n",
      "   macro avg       1.00      1.00      1.00     59598\n",
      "weighted avg       1.00      1.00      1.00     59598\n",
      "\n",
      "Classification Report (Testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.68      0.68      7868\n",
      "           1       0.64      0.65      0.65      7032\n",
      "\n",
      "    accuracy                           0.67     14900\n",
      "   macro avg       0.66      0.67      0.66     14900\n",
      "weighted avg       0.67      0.67      0.67     14900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_dt = dt_model.predict(X_train)\n",
    "y_test_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "train_acc_dt = accuracy_score(y_train, y_train_pred_dt)\n",
    "test_acc_dt = accuracy_score(y_test, y_test_pred_dt)\n",
    "\n",
    "# Reports\n",
    "print(f\"Training Accuracy (Decision Tree): {train_acc_dt:.4f}\")\n",
    "print(f\"Testing Accuracy (Decision Tree): {test_acc_dt:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report (Training):\")\n",
    "print(classification_report(y_train, y_train_pred_dt))\n",
    "\n",
    "print(\"Classification Report (Testing):\")\n",
    "print(classification_report(y_test, y_test_pred_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae95451a-3ec4-4f21-9ccb-874ddc47cd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (Naive Bayes): 0.7171\n",
      "Testing Accuracy (Naive Bayes): 0.7162\n",
      "\n",
      "Classification Report (Training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.73     31260\n",
      "           1       0.70      0.72      0.71     28338\n",
      "\n",
      "    accuracy                           0.72     59598\n",
      "   macro avg       0.72      0.72      0.72     59598\n",
      "weighted avg       0.72      0.72      0.72     59598\n",
      "\n",
      "Classification Report (Testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.73      7868\n",
      "           1       0.69      0.72      0.71      7032\n",
      "\n",
      "    accuracy                           0.72     14900\n",
      "   macro avg       0.72      0.72      0.72     14900\n",
      "weighted avg       0.72      0.72      0.72     14900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_nb = nb_model.predict(X_train)\n",
    "y_test_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "train_acc_nb = accuracy_score(y_train, y_train_pred_nb)\n",
    "test_acc_nb = accuracy_score(y_test, y_test_pred_nb)\n",
    "\n",
    "# Reports\n",
    "print(f\"Training Accuracy (Naive Bayes): {train_acc_nb:.4f}\")\n",
    "print(f\"Testing Accuracy (Naive Bayes): {test_acc_nb:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report (Training):\")\n",
    "print(classification_report(y_train, y_train_pred_nb))\n",
    "\n",
    "print(\"Classification Report (Testing):\")\n",
    "print(classification_report(y_test, y_test_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "885b4ee5-bd11-439d-9d5f-bd14b40ed025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy (KNN): 0.7665\n",
      "Testing Accuracy (KNN): 0.6437\n",
      "\n",
      "Classification Report (Training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78     31260\n",
      "           1       0.76      0.75      0.75     28338\n",
      "\n",
      "    accuracy                           0.77     59598\n",
      "   macro avg       0.77      0.77      0.77     59598\n",
      "weighted avg       0.77      0.77      0.77     59598\n",
      "\n",
      "Classification Report (Testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66      7868\n",
      "           1       0.62      0.62      0.62      7032\n",
      "\n",
      "    accuracy                           0.64     14900\n",
      "   macro avg       0.64      0.64      0.64     14900\n",
      "weighted avg       0.64      0.64      0.64     14900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_knn = knn_model.predict(X_train)\n",
    "y_test_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "train_acc_knn = accuracy_score(y_train, y_train_pred_knn)\n",
    "test_acc_knn = accuracy_score(y_test, y_test_pred_knn)\n",
    "\n",
    "# Reports\n",
    "print(f\"Training Accuracy (KNN): {train_acc_knn:.4f}\")\n",
    "print(f\"Testing Accuracy (KNN): {test_acc_knn:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report (Training):\")\n",
    "print(classification_report(y_train, y_train_pred_knn))\n",
    "\n",
    "print(\"Classification Report (Testing):\")\n",
    "print(classification_report(y_test, y_test_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1285421-3fd7-4111-b584-4c17e4d2663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy (CatBoost): 0.8061176549548642\n",
      "Test Accuracy (CatBoost): 0.7606040268456375\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "cat_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_cat = cat_model.predict(X_train)\n",
    "y_test_pred_cat = cat_model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy (CatBoost):\", accuracy_score(y_train, y_train_pred_cat))\n",
    "print(\"Test Accuracy (CatBoost):\", accuracy_score(y_test, y_test_pred_cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82aaa4a8-8320-42aa-b7ba-d92179daeab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best Parameters:\n",
      " {'depth': 4, 'iterations': 200, 'l2_leaf_reg': 5, 'learning_rate': 0.1}\n",
      "\n",
      "=== Training Evaluation ===\n",
      "Accuracy: 0.7618376455585758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77     31260\n",
      "           1       0.75      0.75      0.75     28338\n",
      "\n",
      "    accuracy                           0.76     59598\n",
      "   macro avg       0.76      0.76      0.76     59598\n",
      "weighted avg       0.76      0.76      0.76     59598\n",
      "\n",
      "\n",
      "=== Testing Evaluation ===\n",
      "Accuracy: 0.7625503355704698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.78      7868\n",
      "           1       0.75      0.75      0.75      7032\n",
      "\n",
      "    accuracy                           0.76     14900\n",
      "   macro avg       0.76      0.76      0.76     14900\n",
      "weighted avg       0.76      0.76      0.76     14900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define the model\n",
    "cat_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'iterations': [100, 200],\n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=cat_model,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,  # 5-fold cross-validation\n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best Parameters\n",
    "print(\"Best Parameters:\\n\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_cat_model = grid_search.best_estimator_\n",
    "\n",
    "y_train_pred = best_cat_model.predict(X_train)\n",
    "y_test_pred = best_cat_model.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Training Evaluation ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\n=== Testing Evaluation ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc605f2-7d62-46d3-b804-24ec6c573034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy (Logistic Regression): 0.7289506359273801\n",
      "Test Accuracy (Logistic Regression): 0.7315436241610739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_log = log_model.predict(X_train)\n",
    "y_test_pred_log = log_model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy (Logistic Regression):\", accuracy_score(y_train, y_train_pred_log))\n",
    "print(\"Test Accuracy (Logistic Regression):\", accuracy_score(y_test, y_test_pred_log))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c5f4253-4d40-4117-b93c-a26b081a8f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy (SVM): 0.7504614248800295\n",
      "Test Accuracy (SVM): 0.7432214765100671\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='rbf', probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_svm = svm_model.predict(X_train)\n",
    "y_test_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy (SVM):\", accuracy_score(y_train, y_train_pred_svm))\n",
    "print(\"Test Accuracy (SVM):\", accuracy_score(y_test, y_test_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b745edc-dd56-4409-b628-dc07ecd3ca62",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "### 1. Random Forest\n",
    "### 2. AdaBoost\n",
    "### 3. Gradient Boosting\n",
    "### 4. The meta-classifier (final decision-maker) will be Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51be4cc1-5751-432a-b000-59c2c986558f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7604026845637584,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.77      0.78      0.78      7868\\n           1       0.75      0.74      0.74      7032\\n\\n    accuracy                           0.76     14900\\n   macro avg       0.76      0.76      0.76     14900\\nweighted avg       0.76      0.76      0.76     14900\\n')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Define base models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define meta-classifier\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Prepare cross-validation for stacking\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize arrays to store predictions\n",
    "train_meta_features = np.zeros((X_train.shape[0], 3))\n",
    "test_meta_features = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "# Train base models using cross-validation\n",
    "for i, model in enumerate([rf, gb, ada]):\n",
    "    test_fold_predictions = np.zeros((X_test.shape[0], 5))  # Store test predictions for each fold\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        model.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "        \n",
    "        # Predict on validation set\n",
    "        train_meta_features[val_idx, i] = model.predict(X_train.iloc[val_idx])\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_fold_predictions[:, fold] = model.predict(X_test)\n",
    "    \n",
    "    # Average test predictions across folds\n",
    "    test_meta_features[:, i] = test_fold_predictions.mean(axis=1)\n",
    "\n",
    "# Meta-features ready! Next step: Train the meta-classifier.\n",
    "# Train the meta-classifier (Logistic Regression)\n",
    "meta_model.fit(train_meta_features, y_train)\n",
    "\n",
    "# Predict on the test meta-features\n",
    "stacking_predictions = meta_model.predict(test_meta_features)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "stacking_accuracy = accuracy_score(y_test, stacking_predictions)\n",
    "stacking_report = classification_report(y_test, stacking_predictions)\n",
    "\n",
    "stacking_accuracy, stacking_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fe27dee-7ddf-4ab6-999a-0b472b895643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7561663143058492\n",
      "Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77     31260\n",
      "           1       0.75      0.74      0.74     28338\n",
      "\n",
      "    accuracy                           0.76     59598\n",
      "   macro avg       0.76      0.76      0.76     59598\n",
      "weighted avg       0.76      0.76      0.76     59598\n",
      "\n",
      "Testing Accuracy: 0.7604026845637584\n",
      "Testing Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.78      7868\n",
      "           1       0.75      0.74      0.74      7032\n",
      "\n",
      "    accuracy                           0.76     14900\n",
      "   macro avg       0.76      0.76      0.76     14900\n",
      "weighted avg       0.76      0.76      0.76     14900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Predict on training meta-features\n",
    "train_stacking_predictions = meta_model.predict(train_meta_features)\n",
    "\n",
    "# Predict on testing meta-features (already done above)\n",
    "test_stacking_predictions = stacking_predictions\n",
    "\n",
    "# Training accuracy and classification report\n",
    "stacking_train_accuracy = accuracy_score(y_train, train_stacking_predictions)\n",
    "stacking_train_report = classification_report(y_train, train_stacking_predictions)\n",
    "\n",
    "# Testing accuracy and classification report\n",
    "stacking_test_accuracy = accuracy_score(y_test, test_stacking_predictions)\n",
    "stacking_test_report = classification_report(y_test, test_stacking_predictions)\n",
    "\n",
    "# Display results\n",
    "print(\"Training Accuracy:\", stacking_train_accuracy)\n",
    "print(\"Training Classification Report:\\n\", stacking_train_report)\n",
    "\n",
    "print(\"Testing Accuracy:\", stacking_test_accuracy)\n",
    "print(\"Testing Classification Report:\\n\", stacking_test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93b06775-5bab-4610-885f-dd77021ebd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7583979328165374\n",
      "Training Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77     31260\n",
      "           1       0.75      0.74      0.74     28338\n",
      "\n",
      "    accuracy                           0.76     59598\n",
      "   macro avg       0.76      0.76      0.76     59598\n",
      "weighted avg       0.76      0.76      0.76     59598\n",
      "\n",
      "Testing Accuracy: 0.7561073825503356\n",
      "Testing Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      7868\n",
      "           1       0.74      0.74      0.74      7032\n",
      "\n",
      "    accuracy                           0.76     14900\n",
      "   macro avg       0.76      0.75      0.76     14900\n",
      "weighted avg       0.76      0.76      0.76     14900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the model\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "train_preds = gb.predict(X_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "test_preds = gb.predict(X_test)\n",
    "\n",
    "# Evaluate training accuracy and classification report\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "train_report = classification_report(y_train, train_preds)\n",
    "\n",
    "# Evaluate testing accuracy and classification report\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "test_report = classification_report(y_test, test_preds)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Training Classification Report:\\n\", train_report)\n",
    "\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "print(\"Testing Classification Report:\\n\", test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f19a5cff-7cea-4e18-860d-9168df1086df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained stacking model\n",
    "joblib.dump(meta_model, \"stacking_model.pkl\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44bdba6c-aa15-4953-b773-fea0e23131bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained stacking model\n",
    "joblib.dump(RandomForestClassifier, \"RandomForestClassifier.pkl\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceee102b-e292-411c-90bd-24e2de27f376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained stacking model\n",
    "joblib.dump(GradientBoostingClassifier, \"GradientBoostingClassifier.pkl\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fe62d28-8c87-40d8-99af-be3101817e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained stacking model\n",
    "joblib.dump(AdaBoostClassifier, \"AdaBoostClassifier.pkl\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8c369c3-5f39-426a-8436-7256ae4a4051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n"
     ]
    }
   ],
   "source": [
    "print(type(rf))  # Check the loaded object type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e50546e0-21a1-48bb-8be9-b7f95e75b3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RandomForestClassifier.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf, \"RandomForestClassifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6700f0c-e710-446e-93b0-90e4ba0eb128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7613422818791946,\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.77      0.78      0.78      7868\\n           1       0.75      0.74      0.75      7032\\n\\n    accuracy                           0.76     14900\\n   macro avg       0.76      0.76      0.76     14900\\nweighted avg       0.76      0.76      0.76     14900\\n')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Define base models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define MLPClassifier as the meta-classifier\n",
    "meta_model = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42)\n",
    "\n",
    "# Prepare cross-validation for stacking\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize arrays to store predictions\n",
    "train_meta_features = np.zeros((X_train.shape[0], 3))\n",
    "test_meta_features = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "# Train base models using cross-validation\n",
    "for i, model in enumerate([rf, gb, ada]):\n",
    "    test_fold_predictions = np.zeros((X_test.shape[0], 5))  # Store test predictions for each fold\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        model.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "        \n",
    "        # Predict on validation set\n",
    "        train_meta_features[val_idx, i] = model.predict(X_train.iloc[val_idx])\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_fold_predictions[:, fold] = model.predict(X_test)\n",
    "    \n",
    "    # Average test predictions across folds\n",
    "    test_meta_features[:, i] = test_fold_predictions.mean(axis=1)\n",
    "\n",
    "# Train the MLP meta-classifier\n",
    "meta_model.fit(train_meta_features, y_train)\n",
    "\n",
    "# Predict on the test meta-features\n",
    "stacking_predictions = meta_model.predict(test_meta_features)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "stacking_accuracy = accuracy_score(y_test, stacking_predictions)\n",
    "stacking_report = classification_report(y_test, stacking_predictions)\n",
    "\n",
    "stacking_accuracy, stacking_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12570477-e793-46ef-b8b5-f355777acd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgadg\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 39\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m meta_model\u001b[38;5;241m.\u001b[39mfit(train_meta_features, y_train)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Predict using Gemini API instead of MLP Meta-Learner\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m gemini_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([gemini_predict(features) \u001b[38;5;28;01mfor\u001b[39;00m features \u001b[38;5;129;01min\u001b[39;00m test_meta_features])\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Evaluate Gemini-based model\u001b[39;00m\n\u001b[0;32m     59\u001b[0m stacking_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, gemini_predictions)\n",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m, in \u001b[0;36mgemini_predict\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m     13\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredict the class label for the following feature set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-1.5-pro-latest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(prompt)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip())  \u001b[38;5;66;03m# Convert output to integer label\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\google\\generativeai\\generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m    332\u001b[0m             request,\n\u001b[0;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[0;32m    334\u001b[0m         )\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:835\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 835\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[0;32m    836\u001b[0m     request,\n\u001b[0;32m    837\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[0;32m    838\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    839\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    840\u001b[0m )\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[0;32m    294\u001b[0m     target,\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[0;32m    296\u001b[0m     sleep_generator,\n\u001b[0;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[0;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     _retry_error_helper(\n\u001b[0;32m    154\u001b[0m         exc,\n\u001b[0;32m    155\u001b[0m         deadline,\n\u001b[0;32m    156\u001b[0m         sleep,\n\u001b[0;32m    157\u001b[0m         error_list,\n\u001b[0;32m    158\u001b[0m         predicate,\n\u001b[0;32m    159\u001b[0m         on_error,\n\u001b[0;32m    160\u001b[0m         exception_factory,\n\u001b[0;32m    161\u001b[0m         timeout,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, retry_delay {\n  seconds: 39\n}\n]"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Configure Gemini API\n",
    "GEMINI_API_KEY = \"AIzaSyBGGcu0nbnTtfZv2OzfIH8GzwcjlosZV_0\"\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "def gemini_predict(features):\n",
    "    \"\"\"Use Gemini API to make predictions based on input features.\"\"\"\n",
    "    prompt = f\"Predict the class label for the following feature set: {features.tolist()}\"\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
    "    response = model.generate_content(prompt)\n",
    "    try:\n",
    "        return int(response.text.strip())  # Convert output to integer label\n",
    "    except ValueError:\n",
    "        return 0  # Default fallback class if conversion fails\n",
    "\n",
    "# Define base models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "ada = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Define MLPClassifier as the meta-classifier\n",
    "meta_model = MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42)\n",
    "\n",
    "# Prepare cross-validation for stacking\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize arrays to store predictions\n",
    "train_meta_features = np.zeros((X_train.shape[0], 3))\n",
    "test_meta_features = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "# Train base models using cross-validation\n",
    "for i, model in enumerate([rf, gb, ada]):\n",
    "    test_fold_predictions = np.zeros((X_test.shape[0], 5))  # Store test predictions for each fold\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "        model.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "        \n",
    "        # Predict on validation set\n",
    "        train_meta_features[val_idx, i] = model.predict(X_train.iloc[val_idx])\n",
    "        \n",
    "        # Predict on test set\n",
    "        test_fold_predictions[:, fold] = model.predict(X_test)\n",
    "    \n",
    "    # Average test predictions across folds\n",
    "    test_meta_features[:, i] = test_fold_predictions.mean(axis=1)\n",
    "\n",
    "# Train the MLP meta-classifier\n",
    "meta_model.fit(train_meta_features, y_train)\n",
    "\n",
    "# Predict using Gemini API instead of MLP Meta-Learner\n",
    "gemini_predictions = np.array([gemini_predict(features) for features in test_meta_features])\n",
    "\n",
    "# Evaluate Gemini-based model\n",
    "stacking_accuracy = accuracy_score(y_test, gemini_predictions)\n",
    "stacking_report = classification_report(y_test, gemini_predictions)\n",
    "\n",
    "stacking_accuracy, stacking_report\n",
    "\n",
    "# ================================\n",
    "# Previous Stacking Model (Commented Out)\n",
    "# ================================\n",
    "# meta_model.fit(train_meta_features, y_train)\n",
    "# stacking_predictions = meta_model.predict(test_meta_features)\n",
    "# stacking_accuracy = accuracy_score(y_test, stacking_predictions)\n",
    "# stacking_report = classification_report(y_test, stacking_predictions)\n",
    "# stacking_accuracy, stacking_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98058d93-1233-4879-ad98-0d3845ee2388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.generativeai\n",
      "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google.generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google.generativeai)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google.generativeai)\n",
      "  Downloading google_api_python_client-2.166.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-auth>=2.15.0 (from google.generativeai)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from google.generativeai) (5.29.2)\n",
      "Collecting pydantic (from google.generativeai)\n",
      "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from google.generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from google.generativeai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google.generativeai)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from google-api-core->google.generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from google-auth>=2.15.0->google.generativeai) (5.5.2)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google.generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google.generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google.generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->google.generativeai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.0 (from pydantic->google.generativeai)\n",
      "  Downloading pydantic_core-2.33.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->google.generativeai)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from tqdm->google.generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.69.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai) (3.2.1)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kgadg\\anaconda3\\envs\\deep_learning\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2025.1.31)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Downloading google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.3 MB 621.2 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 0.8/1.3 MB 838.9 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.3 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading google_api_python_client-2.166.0-py2.py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.2 MB 3.4 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.8/13.2 MB 4.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.9/13.2 MB 5.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.7/13.2 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.6/13.2 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 8.1/13.2 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.0/13.2 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.3/13.2 MB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 7.6 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "Downloading pydantic_core-2.33.0-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 1.8/2.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 9.9 MB/s eta 0:00:00\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 1.8/4.3 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.9/4.3 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: uritemplate, typing-inspection, pydantic-core, pyasn1, proto-plus, httplib2, grpcio, googleapis-common-protos, annotated-types, rsa, pydantic, pyasn1-modules, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google.generativeai\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.69.0\n",
      "    Uninstalling grpcio-1.69.0:\n",
      "      Successfully uninstalled grpcio-1.69.0\n",
      "Successfully installed annotated-types-0.7.0 google-ai-generativelanguage-0.6.15 google-api-core-2.24.2 google-api-python-client-2.166.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google.generativeai-0.8.4 googleapis-common-protos-1.69.2 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 proto-plus-1.26.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.1 pydantic-core-2.33.0 rsa-4.9 typing-inspection-0.4.0 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "968961e6-ff23-44ae-8f69-668a2045703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7fada73-662a-44c2-a365-f5420cf660c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Updated Feature Engineering\n",
    "def enhanced_feature_engineering(train_df, test_df, mcdm_weights):\n",
    "    # Create copies to avoid modifying originals\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_features = ['Gender', 'Job Role', 'Work-Life Balance', 'Job Satisfaction',\n",
    "                           'Performance Rating', 'Education Level', 'Marital Status',\n",
    "                           'Company Size', 'Company Reputation', 'Employee Recognition', \n",
    "                           'Remote Work', 'Leadership Opportunities', 'Innovation Opportunities',\n",
    "                           'Overall_Sentiment_Label']\n",
    "    \n",
    "    numerical_features = ['Age', 'Years at Company', 'Monthly Income', 'Number of Promotions',\n",
    "                         'Distance from Home', 'Job Level', 'Company Tenure', 'Overall_Sentiment_Score']\n",
    "    \n",
    "    # Encode categorical features\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        if col in train.columns and col in test.columns:\n",
    "            le = LabelEncoder()\n",
    "            train[col] = le.fit_transform(train[col])\n",
    "            test[col] = le.transform(test[col])\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Handle Job Level separately if it contains non-numeric values\n",
    "    if 'Job Level' in train.columns and not pd.api.types.is_numeric_dtype(train['Job Level']):\n",
    "        le_job_level = LabelEncoder()\n",
    "        train['Job Level'] = le_job_level.fit_transform(train['Job Level'])\n",
    "        test['Job Level'] = le_job_level.transform(test['Job Level'])\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    train[numerical_features] = scaler.fit_transform(train[numerical_features])\n",
    "    test[numerical_features] = scaler.transform(test[numerical_features])\n",
    "    \n",
    "    # Create interaction features between high-weight features\n",
    "    top_features = sorted(mcdm_weights.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    top_feature_names = [feat[0] for feat in top_features]\n",
    "    \n",
    "    for i, feat1 in enumerate(top_feature_names):\n",
    "        if feat1 in train.columns and feat1 in test.columns:\n",
    "            for j, feat2 in enumerate(top_feature_names[i+1:], i+1):\n",
    "                if feat2 in train.columns and feat2 in test.columns:\n",
    "                    interaction_name = f\"{feat1}_x_{feat2}\"\n",
    "                    train[interaction_name] = train[feat1] * train[feat2]\n",
    "                    test[interaction_name] = test[feat1] * test[feat2]\n",
    "    \n",
    "    # Create polynomial features for top 3 features\n",
    "    top3_features = [feat[0] for feat in top_features[:3]]\n",
    "    valid_top3 = [f for f in top3_features if f in train.columns and f in test.columns]\n",
    "    \n",
    "    if valid_top3:\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "        train_poly = poly.fit_transform(train[valid_top3])\n",
    "        test_poly = poly.transform(test[valid_top3])\n",
    "        \n",
    "        poly_feature_names = [f\"poly_{i}\" for i in range(train_poly.shape[1])]\n",
    "        train_poly_df = pd.DataFrame(train_poly, columns=poly_feature_names)\n",
    "        test_poly_df = pd.DataFrame(test_poly, columns=poly_feature_names)\n",
    "        \n",
    "        train = pd.concat([train, train_poly_df], axis=1)\n",
    "        test = pd.concat([test, test_poly_df], axis=1)\n",
    "    \n",
    "    # Create ratio features\n",
    "    if 'Monthly Income' in train.columns and 'Years at Company' in train.columns:\n",
    "        train['Income_per_Year'] = train['Monthly Income'] / (train['Years at Company'] + 1)  # +1 to avoid division by zero\n",
    "        test['Income_per_Year'] = test['Monthly Income'] / (test['Years at Company'] + 1)\n",
    "    \n",
    "    if 'Number of Promotions' in train.columns and 'Years at Company' in train.columns:\n",
    "        train['Promotion_Rate'] = train['Number of Promotions'] / (train['Years at Company'] + 1)\n",
    "        test['Promotion_Rate'] = test['Number of Promotions'] / (test['Years at Company'] + 1)\n",
    "    \n",
    "    # Apply MCDM weights to features\n",
    "    for feature, weight in mcdm_weights.items():\n",
    "        if feature in train.columns and feature in test.columns:\n",
    "            train[feature] *= weight\n",
    "            test[feature] *= weight\n",
    "    \n",
    "    # Create weighted sentiment features\n",
    "    if 'Overall_Sentiment_Score' in train.columns:\n",
    "        for feat in ['Job Satisfaction', 'Work-Life Balance', 'Performance Rating']:\n",
    "            if feat in train.columns and feat in test.columns:\n",
    "                train[f'{feat}_Sentiment'] = train[feat] * train['Overall_Sentiment_Score']\n",
    "                test[f'{feat}_Sentiment'] = test[feat] * test['Overall_Sentiment_Score']\n",
    "    \n",
    "    # Convert target column to binary\n",
    "    if 'Attrition' in train.columns:\n",
    "        train[\"Attrition\"] = train[\"Attrition\"].apply(lambda x: 1 if x == \"Left\" else 0) if not pd.api.types.is_numeric_dtype(train[\"Attrition\"]) else train[\"Attrition\"]\n",
    "    \n",
    "    if 'Attrition' in test.columns:\n",
    "        test[\"Attrition\"] = test[\"Attrition\"].apply(lambda x: 1 if x == \"Left\" else 0) if not pd.api.types.is_numeric_dtype(test[\"Attrition\"]) else test[\"Attrition\"]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ff0d83-a485-45b1-9cb7-76d269aa9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Feature Selection based on weighted importance\n",
    "def select_features(X_train, y_train, top_n=None):\n",
    "    selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold=-np.inf, max_features=top_n)\n",
    "    selector.fit(X_train, y_train)\n",
    "    return selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee0e0316-16ab-44cf-a21e-0b4b18696bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Advanced Ensemble Model\n",
    "def build_advanced_ensemble(X_train, y_train, X_test, y_test):\n",
    "    # Handle class imbalance with SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Define base models with hyperparameters\n",
    "    rf = RandomForestClassifier(n_estimators=200, max_depth=20, min_samples_split=5, \n",
    "                               min_samples_leaf=2, class_weight='balanced', random_state=42)\n",
    "    \n",
    "    gb = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=8, \n",
    "                                    min_samples_split=5, min_samples_leaf=2, random_state=42)\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=8, \n",
    "                                 subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(n_estimators=200, learning_rate=0.1, max_depth=8,\n",
    "                                  num_leaves=31, subsample=0.8, colsample_bytree=0.8,\n",
    "                                  random_state=42)\n",
    "    \n",
    "    # Neural network for different learning patterns\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam',\n",
    "                       alpha=0.0001, batch_size=256, learning_rate='adaptive',\n",
    "                       max_iter=200, random_state=42)\n",
    "    \n",
    "    # Create voting classifier\n",
    "    voting_clf = VotingClassifier(estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgb', lgb_model),\n",
    "        ('mlp', mlp)\n",
    "    ], voting='soft')\n",
    "    \n",
    "    # Fit all models\n",
    "    print(\"Training Voting Ensemble...\")\n",
    "    voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    y_train_pred = voting_clf.predict(X_train)\n",
    "    y_test_pred = voting_clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    \n",
    "    # Return the trained model and predictions\n",
    "    return voting_clf, y_test_pred, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "971917e2-625a-4621-87f0-02e4c52f09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Stacked Ensemble with Meta-learner\n",
    "def build_stacked_ensemble(X_train, y_train, X_test, y_test):\n",
    "    # Handle class imbalance with SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Define base models\n",
    "    base_models = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, random_state=42)),\n",
    "        ('xgb', xgb.XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=8, random_state=42)),\n",
    "        ('lgb', lgb.LGBMClassifier(n_estimators=200, learning_rate=0.1, max_depth=8, random_state=42)),\n",
    "        ('svm', SVC(probability=True, kernel='rbf', C=1.0, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    # Prepare cross-validation for stacking\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Initialize arrays to store meta-features\n",
    "    n_models = len(base_models)\n",
    "    train_meta_features = np.zeros((X_train.shape[0], n_models))\n",
    "    test_meta_features = np.zeros((X_test.shape[0], n_models))\n",
    "    \n",
    "    # Generate meta-features through cross-validation\n",
    "    print(\"Generating meta-features through cross-validation...\")\n",
    "    for i, (name, model) in enumerate(base_models):\n",
    "        print(f\"Training {name}...\")\n",
    "        test_fold_preds = np.zeros((X_test.shape[0], skf.n_splits))\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_resampled, y_train_resampled)):\n",
    "            # Train on resampled data but use original indices\n",
    "            fold_X_train, fold_y_train = X_train_resampled[train_idx], y_train_resampled[train_idx]\n",
    "            fold_X_val = X_train_resampled[val_idx]\n",
    "            \n",
    "            # Fit model on training fold\n",
    "            model.fit(fold_X_train, fold_y_train)\n",
    "            \n",
    "            # Generate predictions for validation fold\n",
    "            val_indices_in_original = val_idx\n",
    "            train_meta_features[val_indices_in_original, i] = model.predict_proba(X_train.iloc[val_indices_in_original])[:, 1]\n",
    "            \n",
    "            # Generate predictions for test set\n",
    "            test_fold_preds[:, fold] = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Average predictions across folds for test set\n",
    "        test_meta_features[:, i] = test_fold_preds.mean(axis=1)\n",
    "    \n",
    "    # Train meta-learner\n",
    "    print(\"Training meta-learner...\")\n",
    "    meta_learner = LogisticRegression(C=10.0, class_weight='balanced', random_state=42)\n",
    "    meta_learner.fit(train_meta_features, y_train)\n",
    "    \n",
    "    # Make predictions with meta-learner\n",
    "    print(\"Making predictions with meta-learner...\")\n",
    "    stacked_train_preds = meta_learner.predict(train_meta_features)\n",
    "    stacked_test_preds = meta_learner.predict(test_meta_features)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    stacked_train_accuracy = accuracy_score(y_train, stacked_train_preds)\n",
    "    stacked_test_accuracy = accuracy_score(y_test, stacked_test_preds)\n",
    "    \n",
    "    print(f\"Stacked Training Accuracy: {stacked_train_accuracy:.4f}\")\n",
    "    print(f\"Stacked Testing Accuracy: {stacked_test_accuracy:.4f}\")\n",
    "    print(\"\\nStacked Classification Report:\")\n",
    "    print(classification_report(y_test, stacked_test_preds))\n",
    "    \n",
    "    return meta_learner, train_meta_features, test_meta_features, stacked_test_preds, stacked_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e44c5368-03ba-4644-9f98-05130a36ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing feature engineering...\n",
      "Selecting top 38 features...\n",
      "Selected 38 features\n",
      "\n",
      "Building and training voting ensemble...\n",
      "Training Voting Ensemble...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 31260, number of negative: 31260\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8617\n",
      "[LightGBM] [Info] Number of data points in the train set: 62520, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Making predictions...\n",
      "Training Accuracy: 0.8834\n",
      "Testing Accuracy: 0.7536\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.76      7868\n",
      "           1       0.73      0.75      0.74      7032\n",
      "\n",
      "    accuracy                           0.75     14900\n",
      "   macro avg       0.75      0.75      0.75     14900\n",
      "weighted avg       0.75      0.75      0.75     14900\n",
      "\n",
      "\n",
      "Building and training stacked ensemble...\n",
      "Generating meta-features through cross-validation...\n",
      "Training rf...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([    0,     2,     3,     4,     5,     6,     7,     8,     9,    10,\\n       ...\\n       62509, 62510, 62512, 62513, 62514, 62515, 62516, 62517, 62518, 62519],\\n      dtype='int32', length=50016)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 84\u001b[0m\n\u001b[0;32m     66\u001b[0m mcdm_weights \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWork-Life Balance\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.363535\u001b[39m,\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob Satisfaction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.444109\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompany Reputation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.268320\u001b[39m\n\u001b[0;32m     81\u001b[0m }\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Run the main workflow\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m voting_model, stacked_model, blended_preds, blended_accuracy \u001b[38;5;241m=\u001b[39m main(train_df, test_df, mcdm_weights)\n",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(train_df, test_df, mcdm_weights)\u001b[0m\n\u001b[0;32m     30\u001b[0m voting_model, voting_preds, voting_accuracy \u001b[38;5;241m=\u001b[39m build_advanced_ensemble(\n\u001b[0;32m     31\u001b[0m     X_train_selected, y_train, X_test_selected, y_test)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBuilding and training stacked ensemble...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m stacked_model, train_meta, test_meta, stacked_preds, stacked_accuracy \u001b[38;5;241m=\u001b[39m build_stacked_ensemble(\n\u001b[0;32m     35\u001b[0m     X_train_selected, y_train, X_test_selected, y_test)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Final evaluation - compare models\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== Final Results =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m, in \u001b[0;36mbuild_stacked_ensemble\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     28\u001b[0m test_fold_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], skf\u001b[38;5;241m.\u001b[39mn_splits))\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf\u001b[38;5;241m.\u001b[39msplit(X_train_resampled, y_train_resampled)):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Train on resampled data but use original indices\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     fold_X_train, fold_y_train \u001b[38;5;241m=\u001b[39m X_train_resampled[train_idx], y_train_resampled[train_idx]\n\u001b[0;32m     33\u001b[0m     fold_X_val \u001b[38;5;241m=\u001b[39m X_train_resampled[val_idx]\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Fit model on training fold\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Deep_Learning\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index([    0,     2,     3,     4,     5,     6,     7,     8,     9,    10,\\n       ...\\n       62509, 62510, 62512, 62513, 62514, 62515, 62516, 62517, 62518, 62519],\\n      dtype='int32', length=50016)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Step 5: Main workflow\n",
    "def main(train_df, test_df, mcdm_weights):\n",
    "    # Feature engineering\n",
    "    print(\"Performing feature engineering...\")\n",
    "    enhanced_train, enhanced_test = enhanced_feature_engineering(train_df, test_df, mcdm_weights)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_train = enhanced_train.drop(columns=[\"Attrition\", \"Employee ID\"])\n",
    "    y_train = enhanced_train[\"Attrition\"]\n",
    "    X_test = enhanced_test.drop(columns=[\"Attrition\", \"Employee ID\"])\n",
    "    y_test = enhanced_test[\"Attrition\"]\n",
    "    \n",
    "    # Feature selection (optional)\n",
    "    top_n = X_train.shape[1] - 10  # Select all but 10 features\n",
    "    print(f\"Selecting top {top_n} features...\")\n",
    "    selector = select_features(X_train, y_train, top_n=top_n)\n",
    "    \n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    # Convert to DataFrame with feature names\n",
    "    selected_features = X_train.columns[selector.get_support()]\n",
    "    X_train_selected = pd.DataFrame(X_train_selected, columns=selected_features)\n",
    "    X_test_selected = pd.DataFrame(X_test_selected, columns=selected_features)\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features\")\n",
    "    \n",
    "    # Build models\n",
    "    print(\"\\nBuilding and training voting ensemble...\")\n",
    "    voting_model, voting_preds, voting_accuracy = build_advanced_ensemble(\n",
    "        X_train_selected, y_train, X_test_selected, y_test)\n",
    "    \n",
    "    print(\"\\nBuilding and training stacked ensemble...\")\n",
    "    stacked_model, train_meta, test_meta, stacked_preds, stacked_accuracy = build_stacked_ensemble(\n",
    "        X_train_selected, y_train, X_test_selected, y_test)\n",
    "    \n",
    "    # Final evaluation - compare models\n",
    "    print(\"\\n===== Final Results =====\")\n",
    "    print(f\"Voting Ensemble Accuracy: {voting_accuracy:.4f}\")\n",
    "    print(f\"Stacked Ensemble Accuracy: {stacked_accuracy:.4f}\")\n",
    "    \n",
    "    # Blend predictions for final result\n",
    "    print(\"\\nCreating final blended predictions...\")\n",
    "    alpha = 0.6  # Weight for voting ensemble\n",
    "    blend_preds = np.zeros_like(voting_preds, dtype=float)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    voting_probs = voting_model.predict_proba(X_test_selected)[:, 1]\n",
    "    stacked_probs = stacked_model.predict_proba(test_meta)[:, 1]\n",
    "    \n",
    "    # Blend probabilities\n",
    "    blended_probs = alpha * voting_probs + (1-alpha) * stacked_probs\n",
    "    blended_preds = (blended_probs > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate accuracy of blended model\n",
    "    blended_accuracy = accuracy_score(y_test, blended_preds)\n",
    "    print(f\"Blended Ensemble Accuracy: {blended_accuracy:.4f}\")\n",
    "    print(\"\\nBlended Classification Report:\")\n",
    "    print(classification_report(y_test, blended_preds))\n",
    "    \n",
    "    return voting_model, stacked_model, blended_preds, blended_accuracy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the MCDM feature weights\n",
    "    mcdm_weights = {\n",
    "        'Work-Life Balance': 0.363535,\n",
    "        'Job Satisfaction': 0.444109,\n",
    "        'Years at Company': 0.373823,\n",
    "        'Monthly Income': 0.344891,\n",
    "        'Performance Rating': 0.295121,\n",
    "        'Job Level': 0.362679,\n",
    "        'Number of Promotions': 0.397305,\n",
    "        'Distance from Home': 0.315833,\n",
    "        'Overall_Sentiment_Score': 0.248932,\n",
    "        'Company Size': 0.223218,\n",
    "        'Employee Recognition': 0.415096,\n",
    "        'Education Level': 0.301249,\n",
    "        'Company Tenure': 0.226470,\n",
    "        'Company Reputation': 0.268320\n",
    "    }\n",
    "    \n",
    "    # Run the main workflow\n",
    "    voting_model, stacked_model, blended_preds, blended_accuracy = main(train_df, test_df, mcdm_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56036273-5cdf-4eb0-a094-c712122b169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing feature engineering...\n",
      "Selecting top 46 features...\n",
      "Selected 46 features\n",
      "\n",
      "Building and training voting ensemble...\n",
      "Applying SMOTETomek for class balancing...\n",
      "Training Voting Ensemble...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 26104, number of negative: 26104\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9374\n",
      "[LightGBM] [Info] Number of data points in the train set: 52208, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Making predictions...\n",
      "Training Accuracy: 0.9136\n",
      "Testing Accuracy: 0.7517\n",
      "ROC AUC Score: 0.8438\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.76      7868\n",
      "           1       0.73      0.74      0.74      7032\n",
      "\n",
      "    accuracy                           0.75     14900\n",
      "   macro avg       0.75      0.75      0.75     14900\n",
      "weighted avg       0.75      0.75      0.75     14900\n",
      "\n",
      "\n",
      "Building and training stacked ensemble...\n",
      "Applying SMOTE for class balancing...\n",
      "Generating meta-features through cross-validation...\n",
      "Training rf...\n",
      "Training gb...\n",
      "Training xgb...\n",
      "Training lgb...\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 31260, number of negative: 31260\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9362\n",
      "[LightGBM] [Info] Number of data points in the train set: 62520, number of used features: 46\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training svm...\n",
      "Training meta-learner...\n",
      "Making predictions with meta-learner...\n",
      "Stacked Testing Accuracy: 0.7594\n",
      "Stacked ROC AUC Score: 0.8516\n",
      "\n",
      "Stacked Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      7868\n",
      "           1       0.74      0.75      0.75      7032\n",
      "\n",
      "    accuracy                           0.76     14900\n",
      "   macro avg       0.76      0.76      0.76     14900\n",
      "weighted avg       0.76      0.76      0.76     14900\n",
      "\n",
      "\n",
      "Performing hyperparameter tuning for XGBoost...\n",
      "Performing hyperparameter tuning for XGBoost...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best parameters: {'subsample': 0.8, 'scale_pos_weight': 1, 'reg_lambda': 1, 'reg_alpha': 1, 'n_estimators': 300, 'min_child_weight': 1, 'max_depth': 6, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 0.6}\n",
      "Best XGBoost Testing Accuracy: 0.7570\n",
      "Best XGBoost ROC AUC Score: 0.8505\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77      7868\n",
      "           1       0.74      0.75      0.74      7032\n",
      "\n",
      "    accuracy                           0.76     14900\n",
      "   macro avg       0.76      0.76      0.76     14900\n",
      "weighted avg       0.76      0.76      0.76     14900\n",
      "\n",
      "\n",
      "===== Final Results =====\n",
      "Voting Ensemble Accuracy: 0.7517\n",
      "Stacked Ensemble Accuracy: 0.7594\n",
      "Tuned XGBoost Accuracy: 0.7570\n",
      "\n",
      "Creating optimized blended predictions...\n",
      "Optimal blending weights: [0.33333333 0.33333333 0.33333333]\n",
      "Blended Ensemble Accuracy: 0.7597\n",
      "Blended Ensemble AUC: 0.8502\n",
      "\n",
      "Blended Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      7868\n",
      "           1       0.74      0.75      0.75      7032\n",
      "\n",
      "    accuracy                           0.76     14900\n",
      "   macro avg       0.76      0.76      0.76     14900\n",
      "weighted avg       0.76      0.76      0.76     14900\n",
      "\n",
      "\n",
      "Optimizing prediction threshold...\n",
      "Optimal threshold: 0.5122\n",
      "Final Ensemble Accuracy: 0.7603\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77      7868\n",
      "           1       0.75      0.74      0.74      7032\n",
      "\n",
      "    accuracy                           0.76     14900\n",
      "   macro avg       0.76      0.76      0.76     14900\n",
      "weighted avg       0.76      0.76      0.76     14900\n",
      "\n",
      "\n",
      "Top 10 Important Features:\n",
      "                        Feature  Importance\n",
      "13                    Job Level    0.102649\n",
      "11               Marital Status    0.097564\n",
      "16                  Remote Work    0.054365\n",
      "9            Distance from Home    0.043216\n",
      "38              Income_per_Year    0.035108\n",
      "4                Monthly Income    0.034962\n",
      "42                  Age_at_Join    0.034577\n",
      "40           Promo_Tenure_Ratio    0.032985\n",
      "44  Work-Life Balance_Sentiment    0.032561\n",
      "15               Company Tenure    0.032502\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.decomposition import PCA\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Step 1: Enhanced Feature Engineering\n",
    "def enhanced_feature_engineering(train_df, test_df, mcdm_weights):\n",
    "    # Create copies to avoid modifying originals\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_features = ['Gender', 'Job Role', 'Work-Life Balance', 'Job Satisfaction',\n",
    "                           'Performance Rating', 'Education Level', 'Marital Status',\n",
    "                           'Company Size', 'Company Reputation', 'Employee Recognition', \n",
    "                           'Remote Work', 'Leadership Opportunities', 'Innovation Opportunities',\n",
    "                           'Overall_Sentiment_Label']\n",
    "    \n",
    "    numerical_features = ['Age', 'Years at Company', 'Monthly Income', 'Number of Promotions',\n",
    "                         'Distance from Home', 'Job Level', 'Company Tenure', 'Overall_Sentiment_Score']\n",
    "    \n",
    "    # Encode categorical features\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        if col in train.columns and col in test.columns:\n",
    "            le = LabelEncoder()\n",
    "            train[col] = le.fit_transform(train[col])\n",
    "            test[col] = le.transform(test[col])\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # Handle Job Level separately if it contains non-numeric values\n",
    "    if 'Job Level' in train.columns and not pd.api.types.is_numeric_dtype(train['Job Level']):\n",
    "        le_job_level = LabelEncoder()\n",
    "        train['Job Level'] = le_job_level.fit_transform(train['Job Level'])\n",
    "        test['Job Level'] = le_job_level.transform(test['Job Level'])\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    train[numerical_features] = scaler.fit_transform(train[numerical_features])\n",
    "    test[numerical_features] = scaler.transform(test[numerical_features])\n",
    "    \n",
    "    # Create interaction features between high-weight features\n",
    "    top_features = sorted(mcdm_weights.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    top_feature_names = [feat[0] for feat in top_features]\n",
    "    \n",
    "    for i, feat1 in enumerate(top_feature_names):\n",
    "        if feat1 in train.columns and feat1 in test.columns:\n",
    "            for j, feat2 in enumerate(top_feature_names[i+1:], i+1):\n",
    "                if feat2 in train.columns and feat2 in test.columns:\n",
    "                    interaction_name = f\"{feat1}_x_{feat2}\"\n",
    "                    train[interaction_name] = train[feat1] * train[feat2]\n",
    "                    test[interaction_name] = test[feat1] * test[feat2]\n",
    "    \n",
    "    # Create polynomial features for top 3 features\n",
    "    top3_features = [feat[0] for feat in top_features[:3]]\n",
    "    valid_top3 = [f for f in top3_features if f in train.columns and f in test.columns]\n",
    "    \n",
    "    if valid_top3:\n",
    "        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "        train_poly = poly.fit_transform(train[valid_top3])\n",
    "        test_poly = poly.transform(test[valid_top3])\n",
    "        \n",
    "        poly_feature_names = [f\"poly_{i}\" for i in range(train_poly.shape[1])]\n",
    "        train_poly_df = pd.DataFrame(train_poly, columns=poly_feature_names)\n",
    "        test_poly_df = pd.DataFrame(test_poly, columns=poly_feature_names)\n",
    "        \n",
    "        train = pd.concat([train, train_poly_df], axis=1)\n",
    "        test = pd.concat([test, test_poly_df], axis=1)\n",
    "    \n",
    "    # Create ratio features\n",
    "    if 'Monthly Income' in train.columns and 'Years at Company' in train.columns:\n",
    "        train['Income_per_Year'] = train['Monthly Income'] / (train['Years at Company'] + 1)  # +1 to avoid division by zero\n",
    "        test['Income_per_Year'] = test['Monthly Income'] / (test['Years at Company'] + 1)\n",
    "    \n",
    "    if 'Number of Promotions' in train.columns and 'Years at Company' in train.columns:\n",
    "        train['Promotion_Rate'] = train['Number of Promotions'] / (train['Years at Company'] + 1)\n",
    "        test['Promotion_Rate'] = test['Number of Promotions'] / (test['Years at Company'] + 1)\n",
    "        \n",
    "        # New features based on the relationship between promotions and tenure\n",
    "        train['Promo_Tenure_Ratio'] = train['Number of Promotions'] / (train['Company Tenure'] + 1)\n",
    "        test['Promo_Tenure_Ratio'] = test['Number of Promotions'] / (test['Company Tenure'] + 1)\n",
    "    \n",
    "    # Create satisfaction to work ratio\n",
    "    if 'Job Satisfaction' in train.columns and 'Work-Life Balance' in train.columns:\n",
    "        train['Satisfaction_Balance_Ratio'] = train['Job Satisfaction'] / (train['Work-Life Balance'] + 0.1)\n",
    "        test['Satisfaction_Balance_Ratio'] = test['Job Satisfaction'] / (test['Work-Life Balance'] + 0.1)\n",
    "    \n",
    "    # Create age-related features\n",
    "    if 'Age' in train.columns and 'Years at Company' in train.columns:\n",
    "        train['Age_at_Join'] = train['Age'] - train['Years at Company']\n",
    "        test['Age_at_Join'] = test['Age'] - test['Years at Company']\n",
    "    \n",
    "    # Apply MCDM weights to features\n",
    "    for feature, weight in mcdm_weights.items():\n",
    "        if feature in train.columns and feature in test.columns:\n",
    "            train[feature] *= weight\n",
    "            test[feature] *= weight\n",
    "    \n",
    "    # Create weighted sentiment features\n",
    "    if 'Overall_Sentiment_Score' in train.columns:\n",
    "        for feat in ['Job Satisfaction', 'Work-Life Balance', 'Performance Rating']:\n",
    "            if feat in train.columns and feat in test.columns:\n",
    "                train[f'{feat}_Sentiment'] = train[feat] * train['Overall_Sentiment_Score']\n",
    "                test[f'{feat}_Sentiment'] = test[feat] * test['Overall_Sentiment_Score']\n",
    "    \n",
    "    # Convert target column to binary\n",
    "    if 'Attrition' in train.columns:\n",
    "        train[\"Attrition\"] = train[\"Attrition\"].apply(lambda x: 1 if x == \"Left\" else 0) if not pd.api.types.is_numeric_dtype(train[\"Attrition\"]) else train[\"Attrition\"]\n",
    "    \n",
    "    if 'Attrition' in test.columns:\n",
    "        test[\"Attrition\"] = test[\"Attrition\"].apply(lambda x: 1 if x == \"Left\" else 0) if not pd.api.types.is_numeric_dtype(test[\"Attrition\"]) else test[\"Attrition\"]\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Step 2: Feature Selection based on weighted importance\n",
    "def select_features(X_train, y_train, top_n=None):\n",
    "    selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold=-np.inf, max_features=top_n)\n",
    "    selector.fit(X_train, y_train)\n",
    "    return selector\n",
    "\n",
    "# Step 3: Advanced Ensemble Model\n",
    "def build_advanced_ensemble(X_train, y_train, X_test, y_test):\n",
    "    # Handle class imbalance with SMOTETomek (more advanced)\n",
    "    print(\"Applying SMOTETomek for class balancing...\")\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Define base models with hyperparameters\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=300, \n",
    "        max_depth=25, \n",
    "        min_samples_split=4, \n",
    "        min_samples_leaf=1, \n",
    "        class_weight='balanced', \n",
    "        bootstrap=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=250, \n",
    "        learning_rate=0.08, \n",
    "        max_depth=10, \n",
    "        min_samples_split=4, \n",
    "        min_samples_leaf=1,\n",
    "        subsample=0.9,\n",
    "        max_features='sqrt',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=300, \n",
    "        learning_rate=0.08, \n",
    "        max_depth=10, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8, \n",
    "        colsample_bylevel=0.8,\n",
    "        min_child_weight=3,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=1.0,\n",
    "        scale_pos_weight=1.0,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=300, \n",
    "        learning_rate=0.08, \n",
    "        max_depth=10,\n",
    "        num_leaves=40, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=1.0,\n",
    "        min_child_samples=20,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Neural network for different learning patterns\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(120, 80, 40), \n",
    "        activation='relu', \n",
    "        solver='adam',\n",
    "        alpha=0.0005, \n",
    "        batch_size=256, \n",
    "        learning_rate='adaptive',\n",
    "        max_iter=300, \n",
    "        early_stopping=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create voting classifier with calibrated probabilities\n",
    "    voting_clf = VotingClassifier(estimators=[\n",
    "        ('rf', rf),\n",
    "        ('gb', gb),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgb', lgb_model),\n",
    "        ('mlp', mlp)\n",
    "    ], voting='soft', weights=[2, 1.5, 2, 1.5, 1])  # Weight models based on typical performance\n",
    "    \n",
    "    # Fit all models\n",
    "    print(\"Training Voting Ensemble...\")\n",
    "    voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    y_train_pred = voting_clf.predict(X_train)\n",
    "    y_test_pred = voting_clf.predict(X_test)\n",
    "    y_test_proba = voting_clf.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate accuracy and AUC\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    \n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    \n",
    "    # Return the trained model and predictions\n",
    "    return voting_clf, y_test_pred, y_test_proba, test_accuracy\n",
    "\n",
    "# Step 4: Fixed Stacked Ensemble with Meta-learner\n",
    "def build_stacked_ensemble(X_train, y_train, X_test, y_test):\n",
    "    # Handle class imbalance with SMOTE\n",
    "    print(\"Applying SMOTE for class balancing...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Define base models\n",
    "    base_models = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, random_state=42)),\n",
    "        ('xgb', xgb.XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=8, random_state=42, use_label_encoder=False, eval_metric='logloss')),\n",
    "        ('lgb', lgb.LGBMClassifier(n_estimators=200, learning_rate=0.1, max_depth=8, random_state=42)),\n",
    "        ('svm', SVC(probability=True, kernel='rbf', C=1.0, random_state=42))\n",
    "    ]\n",
    "    \n",
    "    # Generate meta-features using cross_val_predict\n",
    "    print(\"Generating meta-features through cross-validation...\")\n",
    "    n_models = len(base_models)\n",
    "    train_meta_features = np.zeros((X_train_resampled.shape[0], n_models))\n",
    "    test_meta_features = np.zeros((X_test.shape[0], n_models))\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Generate meta-features for training set\n",
    "    for i, (name, model) in enumerate(base_models):\n",
    "        print(f\"Training {name}...\")\n",
    "        # Generate predictions for each fold using cross-validation\n",
    "        train_meta_features[:, i] = cross_val_predict(\n",
    "            model, X_train_resampled, y_train_resampled, \n",
    "            cv=skf, method='predict_proba', n_jobs=-1\n",
    "        )[:, 1]\n",
    "        \n",
    "        # Train the model on the entire resampled training data\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        # Generate predictions for test set\n",
    "        test_meta_features[:, i] = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Train meta-learner\n",
    "    print(\"Training meta-learner...\")\n",
    "    meta_learner = LogisticRegression(C=10.0, class_weight='balanced', random_state=42)\n",
    "    meta_learner.fit(train_meta_features, y_train_resampled)\n",
    "    \n",
    "    # Make predictions with meta-learner\n",
    "    print(\"Making predictions with meta-learner...\")\n",
    "    # We need to map our predictions back to the original training set for fair comparison\n",
    "    # For this simplified example, we'll just use the meta-learner on the test set\n",
    "    stacked_test_preds = meta_learner.predict(test_meta_features)\n",
    "    stacked_test_proba = meta_learner.predict_proba(test_meta_features)[:, 1]\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    stacked_test_accuracy = accuracy_score(y_test, stacked_test_preds)\n",
    "    roc_auc = roc_auc_score(y_test, stacked_test_proba)\n",
    "    \n",
    "    print(f\"Stacked Testing Accuracy: {stacked_test_accuracy:.4f}\")\n",
    "    print(f\"Stacked ROC AUC Score: {roc_auc:.4f}\")\n",
    "    print(\"\\nStacked Classification Report:\")\n",
    "    print(classification_report(y_test, stacked_test_preds))\n",
    "    \n",
    "    return meta_learner, train_meta_features, test_meta_features, stacked_test_preds, stacked_test_proba, stacked_test_accuracy\n",
    "\n",
    "# Step 5: Advanced Hyperparameter Tuning for XGBoost\n",
    "def tune_xgboost(X_train, y_train, X_test, y_test):\n",
    "    print(\"Performing hyperparameter tuning for XGBoost...\")\n",
    "    \n",
    "    # Apply SMOTE for class balancing\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Define parameter grid for XGBoost\n",
    "    param_dist = {\n",
    "        'n_estimators': [300, 500, 700],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [6, 8, 10, 12],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "        'reg_lambda': [0.1, 0.5, 1, 5],\n",
    "        'scale_pos_weight': [1, 3, 5]\n",
    "    }\n",
    "    \n",
    "    # Initialize XGBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', \n",
    "                                 use_label_encoder=False, \n",
    "                                 eval_metric='logloss',\n",
    "                                 random_state=42)\n",
    "    \n",
    "    # Set up random search\n",
    "    n_iter_search = 30  # Number of parameter combinations to try\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model, \n",
    "        param_distributions=param_dist, \n",
    "        n_iter=n_iter_search,\n",
    "        scoring='roc_auc',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Perform random search\n",
    "    random_search.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    \n",
    "    print(f\"Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"Best XGBoost Testing Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Best XGBoost ROC AUC Score: {roc_auc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    \n",
    "    return best_model, y_test_pred, y_test_proba, test_accuracy\n",
    "\n",
    "# Step 6: Ensemble Blending with Optimization\n",
    "def optimize_blend_weights(base_probas, y_true):\n",
    "    \"\"\"\n",
    "    Find optimal weights for blending multiple model probabilities\n",
    "    \"\"\"\n",
    "    from scipy.optimize import minimize\n",
    "    \n",
    "    def objective(weights, probas, y_true):\n",
    "        # Normalize weights to sum to 1\n",
    "        weights = weights / np.sum(weights)\n",
    "        \n",
    "        # Calculate blended probabilities\n",
    "        blended_probas = np.zeros(y_true.shape)\n",
    "        for i, w in enumerate(weights):\n",
    "            blended_probas += w * probas[i]\n",
    "        \n",
    "        # Convert to binary predictions\n",
    "        blended_preds = (blended_probas > 0.5).astype(int)\n",
    "        \n",
    "        # Return negative accuracy (to minimize)\n",
    "        return -accuracy_score(y_true, blended_preds)\n",
    "    \n",
    "    # Initialize with equal weights\n",
    "    n_models = len(base_probas)\n",
    "    initial_weights = np.ones(n_models) / n_models\n",
    "    \n",
    "    # Constraints: weights sum to 1\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    \n",
    "    # Bounds: weights between 0 and 1\n",
    "    bounds = [(0, 1) for _ in range(n_models)]\n",
    "    \n",
    "    # Optimize\n",
    "    result = minimize(\n",
    "        objective, \n",
    "        initial_weights, \n",
    "        args=(base_probas, y_true),\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints\n",
    "    )\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    optimal_weights = result.x / np.sum(result.x)\n",
    "    return optimal_weights\n",
    "\n",
    "# Step 7: Main workflow with advanced techniques\n",
    "def main(train_df, test_df, mcdm_weights):\n",
    "    # Feature engineering\n",
    "    print(\"Performing feature engineering...\")\n",
    "    enhanced_train, enhanced_test = enhanced_feature_engineering(train_df, test_df, mcdm_weights)\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X_train = enhanced_train.drop(columns=[\"Attrition\", \"Employee ID\"])\n",
    "    y_train = enhanced_train[\"Attrition\"]\n",
    "    X_test = enhanced_test.drop(columns=[\"Attrition\", \"Employee ID\"])\n",
    "    y_test = enhanced_test[\"Attrition\"]\n",
    "    \n",
    "    # Feature selection\n",
    "    top_n = X_train.shape[1] - 5  # Select all but 5 features\n",
    "    print(f\"Selecting top {top_n} features...\")\n",
    "    selector = select_features(X_train, y_train, top_n=top_n)\n",
    "    \n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    # Convert to DataFrame with feature names\n",
    "    selected_features = X_train.columns[selector.get_support()]\n",
    "    X_train_selected = pd.DataFrame(X_train_selected, columns=selected_features)\n",
    "    X_test_selected = pd.DataFrame(X_test_selected, columns=selected_features)\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features\")\n",
    "    \n",
    "    # Build models\n",
    "    print(\"\\nBuilding and training voting ensemble...\")\n",
    "    voting_model, voting_preds, voting_probs, voting_accuracy = build_advanced_ensemble(\n",
    "        X_train_selected, y_train, X_test_selected, y_test)\n",
    "    \n",
    "    print(\"\\nBuilding and training stacked ensemble...\")\n",
    "    stacked_model, train_meta, test_meta, stacked_preds, stacked_probs, stacked_accuracy = build_stacked_ensemble(\n",
    "        X_train_selected, y_train, X_test_selected, y_test)\n",
    "    \n",
    "    print(\"\\nPerforming hyperparameter tuning for XGBoost...\")\n",
    "    xgb_tuned, xgb_preds, xgb_probs, xgb_accuracy = tune_xgboost(\n",
    "        X_train_selected, y_train, X_test_selected, y_test)\n",
    "    \n",
    "    # Final evaluation - compare models\n",
    "    print(\"\\n===== Final Results =====\")\n",
    "    print(f\"Voting Ensemble Accuracy: {voting_accuracy:.4f}\")\n",
    "    print(f\"Stacked Ensemble Accuracy: {stacked_accuracy:.4f}\")\n",
    "    print(f\"Tuned XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
    "    \n",
    "    # Create a super ensemble (blending)\n",
    "    print(\"\\nCreating optimized blended predictions...\")\n",
    "    \n",
    "    # Collect all probability predictions\n",
    "    all_probs = [voting_probs, stacked_probs, xgb_probs]\n",
    "    \n",
    "    # Find optimal blending weights\n",
    "    optimal_weights = optimize_blend_weights(all_probs, y_test)\n",
    "    print(f\"Optimal blending weights: {optimal_weights}\")\n",
    "    \n",
    "    # Apply optimal weights\n",
    "    blended_probs = np.zeros_like(voting_probs)\n",
    "    for i, probs in enumerate(all_probs):\n",
    "        blended_probs += optimal_weights[i] * probs\n",
    "    \n",
    "    # Convert to predictions\n",
    "    blended_preds = (blended_probs > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate accuracy of blended model\n",
    "    blended_accuracy = accuracy_score(y_test, blended_preds)\n",
    "    blended_auc = roc_auc_score(y_test, blended_probs)\n",
    "    \n",
    "    print(f\"Blended Ensemble Accuracy: {blended_accuracy:.4f}\")\n",
    "    print(f\"Blended Ensemble AUC: {blended_auc:.4f}\")\n",
    "    print(\"\\nBlended Classification Report:\")\n",
    "    print(classification_report(y_test, blended_preds))\n",
    "    \n",
    "    # Finally, try threshold optimization\n",
    "    print(\"\\nOptimizing prediction threshold...\")\n",
    "    thresholds = np.linspace(0.3, 0.7, 50)\n",
    "    best_threshold = 0.5\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        threshold_preds = (blended_probs > threshold).astype(int)\n",
    "        acc = accuracy_score(y_test, threshold_preds)\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Apply optimal threshold\n",
    "    final_preds = (blended_probs > best_threshold).astype(int)\n",
    "    final_accuracy = accuracy_score(y_test, final_preds)\n",
    "    \n",
    "    print(f\"Optimal threshold: {best_threshold:.4f}\")\n",
    "    print(f\"Final Ensemble Accuracy: {final_accuracy:.4f}\")\n",
    "    print(\"\\nFinal Classification Report:\")\n",
    "    print(classification_report(y_test, final_preds))\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    feature_importances = voting_model.named_estimators_['rf'].feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Important Features:\")\n",
    "    print(feature_importance_df.head(10))\n",
    "    \n",
    "    return voting_model, stacked_model, xgb_tuned, final_preds, final_accuracy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the MCDM feature weights\n",
    "    mcdm_weights = {\n",
    "        'Work-Life Balance': 0.363535,\n",
    "        'Job Satisfaction': 0.444109,\n",
    "        'Years at Company': 0.373823,\n",
    "        'Monthly Income': 0.344891,\n",
    "        'Performance Rating': 0.295121,\n",
    "        'Job Level': 0.362679,\n",
    "        'Number of Promotions': 0.397305,\n",
    "        'Distance from Home': 0.315833,\n",
    "        'Overall_Sentiment_Score': 0.248932,\n",
    "        'Company Size': 0.223218,\n",
    "        'Employee Recognition': 0.415096,\n",
    "        'Education Level': 0.301249,\n",
    "        'Company Tenure': 0.226470,\n",
    "        'Company Reputation': 0.268320\n",
    "    }\n",
    "    \n",
    "    # Run the main workflow\n",
    "    voting_model, stacked_model, xgb_model, final_preds, final_accuracy = main(train_df, test_df, mcdm_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5b879-c84a-4929-9cb1-57159858847a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
